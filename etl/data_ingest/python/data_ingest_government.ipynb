{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import von librarys\n",
    "\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from lxml import etree\n",
    "\n",
    "# Umgebungsvariablen laden (sollten identisch zu den anderen Skripts sein)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "dbname = os.getenv('DB_NAME', 'DB_NAME')\n",
    "user = os.getenv('DB_USER', 'DB_USER')\n",
    "password = os.getenv('DB_PASSWORD', 'DB_PASSWORD')\n",
    "host = os.getenv('DB_HOST', 'DB_HOST')\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "### funktion definieren, um Daten aus diesem ultra hässlichen XML zu extrahieren\n",
    "def fetch_eurostat_data(dataset_code):\n",
    "    url = f\"https://ec.europa.eu/eurostat/api/dissemination/sdmx/3.0/data/dataflow/ESTAT/{dataset_code}/1.0?compress=false\"\n",
    "    print(url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch data for {dataset_code}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    root = etree.fromstring(response.content)\n",
    "\n",
    "    # die Logik ist vermutlich korrekt aber ich bin mir nicht sicher, ob findall hier korrekt ist\n",
    "    structured_data = []\n",
    "    for series in root.findall('.//Series', namespaces={}):\n",
    "        series_data = series.attrib\n",
    "        for obs in series.findall('.//Obs', namespaces={}):\n",
    "            obs_data = obs.attrib\n",
    "            record = {**series_data, **obs_data}\n",
    "            structured_data.append(record) ### Alle Daten werden extrahiert und untereinandergehängt\n",
    "\n",
    "    df = pd.DataFrame(structured_data)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Bitte hinterfrage dein Leben, vermutlich ist der Namespace nicht korrekt\")\n",
    "    else:\n",
    "        print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "data = fetch_eurostat_data('gov_10a_main')\n",
    "\n",
    "\n",
    "\n",
    "# Daten analog zu bisherigen Skripten auf die lokale Datenbank laden\n",
    "# Offenbar kann postgres nicht mi nans umgehen, keine Ahnung warum aber in diesem Skript scheinen nan akzeptabe\n",
    "# Also Aufgabe für Charlotte: NANs rausfiltern\n",
    "\n",
    "\n",
    "def upload_to_postgres(df, table_name):\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}/{dbname}')\n",
    "    df.columns = [col.replace(' ', '_').lower() for col in df.columns] ### Spalten in Tabelle haben gleichen Namen wie Spalten in Dataframe\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "    df.to_sql(table_name, engine, schema='eurostat_stag', if_exists='replace', index=False, chunksize=500)\n",
    "\n",
    "\n",
    "dataset_mapping = {\n",
    "    'gov_10a_main': 'gov_10a_main_table' ### Du kannst die Tabelle natürlich nennen wie du lustig bist\n",
    "}\n",
    "\n",
    "### funktion, die das beste aus allen Welten zusammenfügt\n",
    "for dataset_code, table_name in dataset_mapping.items():\n",
    "    print(f\"\\nDataset: {dataset_code}\")\n",
    "    data = fetch_eurostat_data(dataset_code)\n",
    "    if data is not None:\n",
    "        upload_to_postgres(data, table_name)\n",
    "\n",
    "### Runtime berechnen\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f\"Script runtime: {runtime} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
